{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ca970a",
   "metadata": {},
   "source": [
    "This example assumes there are only 2 parties, and we used a simplified equation for demand:\n",
    "\n",
    "$q_1= 10 - p_1 + 0.5 p_2$,\n",
    "\n",
    "$q_2= 10 - p_2 + 0.5 p_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a14cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'converged': True, 'periods_run': 405000, 'stable_periods': 100000, 'epsilon_final': 0.00030353913807886623}\n",
      "Greedy action at s(8,8): firm1 = 8 , firm2 = 7\n",
      "Greedy action at s(11,11): firm1 = 10 , firm2 = 12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from qlearning_env.constants import RESULTS_DIR\n",
    "\n",
    "PRICES = np.arange(5, 15)  # {5,6,...,14}\n",
    "N_ACTIONS = len(PRICES)\n",
    "N_STATES = N_ACTIONS * N_ACTIONS  # Number of combinations of (p1,p2)\n",
    "price_to_idx = {p:i for i,p in enumerate(PRICES)} # Map price to column index 0-9\n",
    "\n",
    "def state_index(p1, p2):\n",
    "    \"\"\"Flattens a 2D (p1, p2) into a single row index.\"\"\"\n",
    "    return price_to_idx[p1] * N_ACTIONS + price_to_idx[p2]\n",
    "\n",
    "def index_to_state(s: int):\n",
    "    \"\"\"Inverse map from index -> (p1,p2).\"\"\"\n",
    "    i = s // N_ACTIONS\n",
    "    j = s %  N_ACTIONS\n",
    "    return PRICES[i], PRICES[j]\n",
    "\n",
    "\n",
    "def demand1(p1, p2):  # q1\n",
    "    return max(0.0, 10 - p1 + 0.5 * p2)\n",
    "\n",
    "def demand2(p1, p2):  # q2\n",
    "    return max(0.0, 10 - p2 + 0.5 * p1)\n",
    "\n",
    "def profit1(p1, p2, c=2.0):\n",
    "    return (p1 - c) * demand1(p1, p2)\n",
    "\n",
    "def profit2(p1, p2, c=2.0):\n",
    "    return (p2 - c) * demand2(p1, p2)\n",
    "\n",
    "def epsilon_at(step, beta):\n",
    "    return float(np.exp(-beta * step)) # ε_t = exp(-beta * t)\n",
    "\n",
    "def argmax_tie(x):\n",
    "    # When there are more than one max, choose the one with higher index\n",
    "    m = np.max(x)\n",
    "    idxs = np.flatnonzero(np.isclose(x, m))\n",
    "    return np.random.choice(idxs) # choose a random one\n",
    "    # return int(idxs[-1])\n",
    "\n",
    "def greedy_map(Q):\n",
    "    # returns an array of length N_STATES with best-action indices\n",
    "    return np.array([argmax_tie(Q[s_]) for s_ in range(N_STATES)], dtype=int)\n",
    "\n",
    "# ---------- Q-learning training ----------\n",
    "def train_long_episode(\n",
    "    alpha=0.25,\n",
    "    delta=0.95,\n",
    "    beta=2*1e-5,\n",
    "    c=2.0,\n",
    "    stable_required=100_000,  # need greedy policy unchanged for this long\n",
    "    check_every=1_000,        # compare policies every K periods\n",
    "    max_periods=2_000_000,    # hard cap so we won't loop forever\n",
    "    seed=43\n",
    "):\n",
    "    rng = np.random.default_rng(seed) # NumPy random generator with a fixed seed\n",
    "    # Initialize the two Q-tables (one per firm), all value are 0\n",
    "    Q1 = np.zeros((N_STATES, N_ACTIONS))\n",
    "    Q2 = np.zeros((N_STATES, N_ACTIONS))\n",
    "\n",
    "    # Pick a single random starting state\n",
    "    p1, p2 = rng.choice(PRICES), rng.choice(PRICES)\n",
    "    s = state_index(p1, p2)\n",
    "\n",
    "    # For stability checks: Record each firm’s argmax action per row.\n",
    "    prev_pi1 = greedy_map(Q1)\n",
    "    prev_pi2 = greedy_map(Q2)\n",
    "    stable = 0 # stability check counter\n",
    "\n",
    "    for t in range(1, max_periods + 1): # loop over periods/steps\n",
    "        eps = epsilon_at(t, beta=beta)\n",
    "\n",
    "        # ε-greedy choices with deterministic exploitation\n",
    "        if rng.random() < eps:\n",
    "          a1 = rng.integers(0, N_ACTIONS)\n",
    "        else:\n",
    "          a1 = argmax_tie(Q1[s])\n",
    "\n",
    "        if rng.random() < eps:\n",
    "          a2 = rng.integers(0, N_ACTIONS)\n",
    "        else:\n",
    "          a2 = argmax_tie(Q2[s])\n",
    "\n",
    "        # Compute next state when an action is chosen\n",
    "        p1_next, p2_next = PRICES[a1], PRICES[a2] # convert action indices to actual prices\n",
    "        s_next = state_index(p1_next, p2_next) # compute the next state\n",
    "\n",
    "        pi1 = profit1(p1_next, p2_next, c=c)\n",
    "        pi2 = profit2(p1_next, p2_next, c=c)\n",
    "\n",
    "        # Q-learning updates\n",
    "        Q1[s, a1] = (1-alpha) * Q1[s, a1] + alpha * (pi1 + delta * np.max(Q1[s_next]))\n",
    "        Q2[s, a2] = (1-alpha) * Q2[s, a2] + alpha * (pi2 + delta * np.max(Q2[s_next]))\n",
    "\n",
    "        s = s_next\n",
    "\n",
    "        # check policy-stability criterion over all states\n",
    "        # for every check_every periods, build the greedy argmax per state policies for both firms\n",
    "        if t % check_every == 0:\n",
    "            current_pi1 = greedy_map(Q1)\n",
    "            current_pi2 = greedy_map(Q2)\n",
    "            if np.array_equal(current_pi1, prev_pi1) and np.array_equal(current_pi2, prev_pi2):\n",
    "                stable += check_every\n",
    "            else:\n",
    "                stable = 0\n",
    "                prev_pi1, prev_pi2 = current_pi1, current_pi2\n",
    "\n",
    "            if stable >= stable_required:\n",
    "                return Q1, Q2, {\n",
    "                    \"converged\": True,\n",
    "                    \"periods_run\": t,\n",
    "                    \"stable_periods\": stable,\n",
    "                    \"epsilon_final\": eps\n",
    "                }\n",
    "\n",
    "    return Q1, Q2, {\n",
    "        \"converged\": False,\n",
    "        \"periods_run\": max_periods,\n",
    "        \"stable_periods\": stable,\n",
    "        \"epsilon_final\": epsilon_at(max_periods, beta=beta)\n",
    "    }\n",
    "\n",
    "# ====== Run ======\n",
    "Q1, Q2, info = train_long_episode(\n",
    "    alpha=0.125,\n",
    "    delta=0.95,\n",
    "    beta=2*1e-5,\n",
    "    stable_required=100_000,\n",
    "    check_every=1_000,\n",
    "    max_periods=2_000_000,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "print(info)\n",
    "\n",
    "# Inspect the equilibrium fingerprint at s(8,8)\n",
    "s8 = state_index(8, 8)\n",
    "a1_star_8 = argmax_tie(Q1[s8])\n",
    "a2_star_8 = argmax_tie(Q2[s8])\n",
    "print(\"Greedy action at s(8,8): firm1 =\", PRICES[a1_star_8], \", firm2 =\", PRICES[a2_star_8])\n",
    "\n",
    "# Inspect the equilibrium fingerprint at s(11,11)\n",
    "s11 = state_index(11,11)\n",
    "a1_star_11 = argmax_tie(Q1[s11])\n",
    "a2_star_11 = argmax_tie(Q2[s11])\n",
    "print(\"Greedy action at s(11,11): firm1 =\", PRICES[a1_star_11], \", firm2 =\", PRICES[a2_star_11])\n",
    "\n",
    "# Export\n",
    "states = [f\"s({p1},{p2})\" for p1 in PRICES for p2 in PRICES]\n",
    "actions = [f\"price={p}\" for p in PRICES]\n",
    "pd.DataFrame(Q1, index=states, columns=actions).to_csv(RESULTS_DIR / \"Q1.csv\")\n",
    "pd.DataFrame(Q2, index=states, columns=actions).to_csv(RESULTS_DIR / \"Q2.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
